{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads numpy and matplotlib\n",
    "%pylab inline\n",
    "\n",
    "# imports functions to interact with operating system\n",
    "import os\n",
    "\n",
    "import scipy as sp\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot(np.tanh(np.random.randn(10)))\n",
    "def makeRandomWalk(len):\n",
    "    pt = zeros(len)\n",
    "    pt[0] = np.random.randn(1)\n",
    "    for n in range(pt.size-1):\n",
    "        pt[n+1] = pt[n] + np.random.randn(1)\n",
    "    pt = pt - np.min(pt)\n",
    "    pt = (pt / np.max(pt) * 2) - 1\n",
    "    return pt\n",
    "    \n",
    "\n",
    "#plot()\n",
    "pattern1 = makeRandomWalk(20)\n",
    "pattern2 = makeRandomWalk(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(pattern1)\n",
    "plot(pattern2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current working path\n",
    "cwd = os.getcwd()\n",
    "print( cwd )\n",
    "\n",
    "# load gesture 1\n",
    "gesture1 = np.loadtxt(cwd + '/gesturedata_0.txt')\n",
    "gesture2 = np.loadtxt(cwd + '/gesturedata_1.txt')\n",
    "\n",
    "# plot the two gestures\n",
    "figsize(20,8);\n",
    "\n",
    "plot( gesture1 )\n",
    "plot( gesture2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(20,5)\n",
    "# to normalize the two gestures to each other, we find the maximum value in the two arrays:\n",
    "maxValue = max( max(gesture1), max(gesture2) )\n",
    "\n",
    "print( maxValue )\n",
    "\n",
    "# scale the gestures and make bipolar:\n",
    "\n",
    "gesture1 = (gesture1 / maxValue)*2 - 1;\n",
    "gesture2 = (gesture2 / maxValue)*2 - 1;\n",
    "\n",
    "# plot it again: looks the same, but scale on the y-axis is now between -1 and 1\n",
    "plot( gesture1 )\n",
    "plot( gesture2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define some useful functions\n",
    "@jit\n",
    "def nrmse(output,target):\n",
    "    combinedVar = 0.5 * (np.var(target, ddof=1) + np.var(output, ddof=1))\n",
    "    errorSignal = output - target\n",
    "    return np.sqrt(np.mean(errorSignal ** 2) / combinedVar)\n",
    "\n",
    "def generateInternalWeights(nInternalUnits, connectivity):\n",
    "    success = False\n",
    "    internalWeights = 0\n",
    "    while success == False:\n",
    "        try:\n",
    "            internalWeights = np.random.randn(nInternalUnits,nInternalUnits) * (np.random.random((nInternalUnits,nInternalUnits)) < connectivity)\n",
    "            specRad = abs(np.linalg.eig(internalWeights)[0][0])\n",
    "            if (specRad > 0):\n",
    "                internalWeights = internalWeights / specRad\n",
    "                success = True\n",
    "        except e:\n",
    "            print(e)\n",
    "    return internalWeights\n",
    "\n",
    "pLoop = lambda n,p: p[n%p.size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def makeClassifierNetwork(p):\n",
    "    Netconnectivity = 1\n",
    "    if p['N'] > 20:\n",
    "        Netconnectivity = 10.0/p['N'];\n",
    "    WstarRaw = generateInternalWeights(p['N'], Netconnectivity)\n",
    "    WinRaw = 2 * (np.random.rand(p['N'], 1) - 0.5)\n",
    "    WbiasRaw = 2 * (np.random.rand(p['N'], 1) - 0.5)\n",
    "\n",
    "    #Scale raw weights     \n",
    "    Wstar = p['NetSR'] * WstarRaw;\n",
    "    W = Wstar\n",
    "    Win = p['NetinpScaling'] * WinRaw;\n",
    "    Wbias = p['BiasScaling'] * WbiasRaw;  \n",
    "    I = np.eye(p['N'])\n",
    "    xCollector = np.zeros((p['N'], p['learnLength']))\n",
    "    pCollector = np.zeros((1, p['learnLength']))\n",
    "    x = np.zeros((p['N'],1))\n",
    "    \n",
    "    allTrainxArgs = np.zeros((p['N'] + 1, 0));\n",
    "    allTrainOldxArgs = np.zeros((p['N'], 0));\n",
    "    allTrainWtargets = np.zeros((p['N'], 0));\n",
    "    allTrainOuts = np.zeros((1, 0));\n",
    "    patternRs =  np.zeros((1, p['patts'].size), dtype=np.object)\n",
    "    \n",
    "    for i_pattern in range(p['patts'].size):\n",
    "        print('Observing pattern ', i_pattern)\n",
    "        patt = p['patts'][i_pattern]\n",
    "        xCollector = np.zeros((p['N'] + 1, p['learnLength']));\n",
    "        xOldCollector = np.zeros((p['N'], p['learnLength']));\n",
    "        WTargetCollector = np.zeros((p['N'], p['learnLength']));\n",
    "        pCollector = np.zeros((1, p['learnLength']));\n",
    "        x = np.zeros((p['N'], 1));\n",
    "\n",
    "        for n in range(p['washoutLength'] + p['learnLength']):\n",
    "            u = patt(n+1)\n",
    "            xOld = x\n",
    "            Wtarget = (Wstar.dot(x)) + (Win.dot(u))\n",
    "            x = ((1.0-p['LR']) * xOld) + (p['LR'] * tanh(Wtarget + Wbias))\n",
    "            if n >= p['washoutLength']:\n",
    "                xCollector[:, n - p['washoutLength']] = np.concatenate((x[:,0], np.array([1])))\n",
    "                xOldCollector[:, n - p['washoutLength']] = xOld[:,0]\n",
    "                WTargetCollector[:, n - p['washoutLength']] = Wtarget[:,0]\n",
    "                pCollector[0, n - p['washoutLength']] = u\n",
    "            uOld = u\n",
    "        \n",
    "        R = xCollector[0:-1].dot(xCollector[0:-1].T) / p['learnLength']\n",
    "        patternRs[0,i_pattern] = R\n",
    "        allTrainxArgs = np.concatenate((allTrainxArgs, xCollector), axis=1)\n",
    "        allTrainOldxArgs = np.concatenate((allTrainOldxArgs, xOldCollector), axis=1)\n",
    "        allTrainOuts = np.concatenate((allTrainOuts, pCollector), axis=1)\n",
    "        allTrainWtargets = np.concatenate((allTrainWtargets, WTargetCollector), axis=1)\n",
    "\n",
    "    return locals()\n",
    "\n",
    "params = {'N':40, 'NetSR':1.2, 'NetinpScaling':1,'BiasScaling':0.6,\n",
    "         'washoutLength':300, 'learnLength':40, \n",
    "          'LR': 0.6,\n",
    "#           'patts':np.array([lambda x:pLoop(x,p_rw1), lambda x: pLoop(x,p_rw2)])\n",
    "          'patts':np.array([lambda x:pLoop(x,pattern1), lambda x: pLoop(x,pattern2)])\n",
    "         }\n",
    "\n",
    "net = makeClassifierNetwork(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# savedNet = net\n",
    "net = savedNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(20,8)\n",
    "plot(net['allTrainxArgs'].T)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define functions to compute and test the conceptors\n",
    "\n",
    "def computeConceptors(p, net, alphas):\n",
    "    print('Computing conceptors, alphas: ', alphas)\n",
    "    Cs = np.zeros((4, p['patts'].size), dtype=np.object)\n",
    "    for i_pattern in range(p['patts'].size):\n",
    "        R = net['patternRs'][0,i_pattern]\n",
    "        [U,s,V] = svd(R)\n",
    "        S = diag(s)\n",
    "        Snew = (S * linalg.inv(S + pow(alphas[i_pattern], -2) * np.eye(p['N'])))\n",
    "\n",
    "        C =  U.dot(Snew).dot(U.T);\n",
    "        Cs[0,i_pattern] = C\n",
    "        Cs[1,i_pattern] = U\n",
    "        Cs[2,i_pattern] = diag(Snew)\n",
    "        Cs[3,i_pattern] = diag(S)\n",
    "    return locals()\n",
    "\n",
    "import scipy.spatial.distance as dist\n",
    "\n",
    "\n",
    "def plotConceptorPatterns(p, cNet, subr, suboff, recallTestLength):\n",
    "    x_CTestPL = np.zeros((3, recallTestLength, p['patts'].size))\n",
    "    p_CTestPL = np.zeros((1, recallTestLength, p['patts'].size))\n",
    "    cx_CTestPL = np.zeros((p['patts'].size,recallTestLength))\n",
    "    pat_CTestPL = np.zeros((1,recallTestLength))\n",
    "    \n",
    "    for i_pattern in range(p['patts'].size):\n",
    "        patt = p['patts'][i_pattern]\n",
    "        x = np.zeros((p['N'],1))\n",
    "        for n in range(recallTestLength + p['washoutLength']):\n",
    "            u = patt(n+1)\n",
    "            xOld = x\n",
    "            Wtarget = (cNet['net']['W'].dot(x)) + (cNet['net']['Win'].dot(u))\n",
    "            x = ((1.0-p['LR']) * xOld) + (p['LR'] * tanh(Wtarget + cNet['net']['Wbias']))\n",
    "            if (n > p['washoutLength']):\n",
    "                pat_CTestPL[0,n-p['washoutLength'] ] = u\n",
    "                C = cNet['Cs'][0,0]\n",
    "                C2 = cNet['Cs'][0,1]\n",
    "                cx = x.T.dot(C.dot(x)) + x.T.dot((1.0-C2).dot(x))\n",
    "                cx_CTestPL[0,n-p['washoutLength']] = cx\n",
    "                cx2 = x.T.dot(C2.dot(x)) + x.T.dot((1.0-C).dot(x))\n",
    "#                 cx2 = x.T.dot(C2.dot(x))\n",
    "                cx_CTestPL[1,n-p['washoutLength']] = cx2\n",
    "\n",
    "        ax = subplot(subr, 2, suboff + i_pattern + 1)\n",
    "        ax.set_title(\"Sample \" + str(suboff/2) + \", pattern: \" + str(i_pattern) + \", alphas: \" + str(cNet['alphas']))\n",
    "#         plot([p['patts'][i_pattern](x) for x in arange(recallTestLength)])\n",
    "        plot(pat_CTestPL.T)\n",
    "        plot(cx_CTestPL.T)\n",
    "        classificationScore = np.sum(cx_CTestPL[i_pattern] > cx_CTestPL[1 - i_pattern]) / cx_CTestPL[0].size\n",
    "        separation = dist.euclidean(cx_CTestPL[i_pattern],cx_CTestPL[1-i_pattern])\n",
    "        print(\"Pattern \", str(i_pattern), \" classification score: \", classificationScore, \", separation: \", separation)\n",
    "        #testing\n",
    "\n",
    "    return locals()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conceptorSampleSize = 20\n",
    "conceptorNets = np.zeros(conceptorSampleSize, dtype=np.object)\n",
    "figsize(15,conceptorSampleSize * 1.9)\n",
    "for i_cnet in range(conceptorSampleSize):\n",
    "    conceptorNets[i_cnet] = computeConceptors(params, net, np.random.random(2) * 100)\n",
    "    pq = plotConceptorPatterns(params, conceptorNets[i_cnet], conceptorSampleSize, i_cnet * 2, 100)\n",
    "plt.tight_layout();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#which are the best apertures?\n",
    "bestConceptor = conceptorNets[12]\n",
    "import dill as pickle\n",
    "with open(r\"net.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(bestConceptor, output_file, protocol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
